\documentclass[9pt]{article}

\usepackage[margin=2cm]{geometry}
\usepackage{hyperref}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amsmath, amsthm, amssymb}
\usepackage{multicol}
\usepackage{tikz}
\usetikzlibrary{graphs}

\newtheorem{theorem}{Theorem}
\newtheorem{claim}{Claim}
\newtheorem{corollary}{Corollary}

\title{
    \textbf{CSE319: Modern Algorithm Design} \\
    \textbf{\large{Homework 1 Solutions}}
}
\author{
    \textbf{Submitted By:} \href{mailto:divyajeet21529@iiitd.ac.in}{Divyajeet Singh (2021529)}
    \hfill
    \textbf{Discussion Partner:} \href{mailto:siddhant21565@iiitd.ac.in}{Siddhant Rai Viksit (2021565)}
}
\date{}

\begin{document}
\maketitle

\section*{Solution 1.}
The given algorithm can be described by Algorithm (\ref{alg:mod-quicksort}).
\begin{algorithm}
    \caption{Modified Quicksort Algorithm}
    \label{alg:mod-quicksort}
    \begin{algorithmic}[1]
        \Procedure{Mod-Quicksort}{$A[l:r]$}:
            \If{$l \geq r$}
                \State \Return
            \EndIf
            \While{\textsc{True}}
                \State $i \sim l, \dots, r$ uniformly at random
                \State $j \gets \Call{Partition}{A[l:r], i}$
                \If{$j-l+1 \leq \frac{1}{4}(r-l+1)$ \textbf{and} $r-j+1 \leq \frac{3}{4}(r-l+1)$}
                    \State \textbf{break}
                \EndIf
            \EndWhile
            \State $\Call{Mod-Quicksort}{A[l:j-1]}$
            \State $\Call{Mod-Quicksort}{A[j+1:r]}$
        \EndProcedure
    \end{algorithmic}
\end{algorithm}

\subsection*{Part (a)}
To analyze the expected running time of this algorithm, we first bound the probability of
successfully finding a \textit{good pivot}\footnote{\textbf{Definition. \textit{Good Pivot}:} A pivot
having at most $\frac{1}{4}$ of the total elements to one side.}. Let's find the number of good pivots
in an array of size $n$. It is easy to see that if the sorted index $j$ of the pivot is less than
$\frac{1}{4} n$, then more than $\frac{3}{4} n$ elements land to its right. Similarly, if $j > \frac{3}{4} n$,
then more than $\frac{3}{4} n$ elements land to its left. So, we see that the number of good pivots
in an array of size $n$ is given by
\begin{equation}
    \label{good-pivot}
    \left( \frac{3}{4} - \frac{1}{4} \right) n = \frac{n}{2}
\end{equation}
Since the pivot is picked uniformly at random, the probability of picking a good pivot in one trial
is
\begin{equation}
    \label{prob-good-pivot}
    p = \frac{n}{2n} = \frac{1}{2}
\end{equation}
It is not hard to notice that the number of trials required for successfully picking a pivot is
distributed as a random variable $X \sim \Call{Geometric}{p}$ since we repeat the trials (which are
independent) until a good pivot is found. We find the expected number of trials required to get a
successful trial, i.e. find a good pivot.
\begin{equation}
    \label{exp-X}
    \mathbb{E}[X] = p^{-1} = \left(\frac{1}{2} \right)^{-1} = 2
\end{equation}
Note that each time a pivot is picked and tested for \textit{goodness}, it takes $O(n)$ time for
an array of size $n$ since its rank is computed using partitioning (or any other $O(n)$ time algorithm).
Finally, we estimate the rank of a good pivot. We calculate the number of elements $R$ that can land
to the right (or left) of a good pivot. Let $\textbf{G}$ be the event that a good pivot is chosen.
Then, $\textbf{G} = \left\{ \frac{1}{4} n \leq R \leq \frac{3}{4} n \right\}$. Since the pivot is
chosen uniformly at random, we have
\begin{align}
    \label{R2}
    \mathbb{P}[R = i \mid \textbf{G}] &= \begin{cases}
        \frac{2}{n} & \text{if } \frac{1}{4}n \leq i \leq \frac{3}{4}n \\
        0 & \text{otherwise}
    \end{cases} \qquad \text{by \eqref{good-pivot}}\\
    \label{exp-R2}
    \mathbb{E}[R \mid \textbf{G}] = \sum_{i=1}^{n} i \ \mathbb{P}[R = i \mid \textbf{G}]
    = \sum_{i=\frac{1}{4}n}^{\frac{3}{4}n} \frac{2i}{n}
    &= \frac{2}{n} \sum_{i=\frac{1}{4}n}^{\frac{3}{4}n} i
    = \frac{2}{n} \left[ \frac{1}{2} \cdot \left( \frac{n}{2} + 1 \right) \cdot \left( 2 \cdot \frac{n}{4} + \frac{n}{2} \right) \right]
    = \frac{n}{2} + 1 \approx \frac{n}{2}
\end{align}
Similarly, approximately $\frac{n}{2}$ elements land to the left of a good pivot, which means that
the expected rank of a good pivot is $\frac{n}{2}$. Let $T(n)$ denote the time taken by Algorithm
(\ref{alg:mod-quicksort}) on an input of size $n$. The recurrence relation can be defined as follows
(the second term represents the expected work done to find the pivot).
\begin{equation}
    \label{exp-Tn-mod-quicksort}
    \begin{split}
        \mathbb{E}[T(n)] &= 2 \mathbb{E}_{k \sim \frac{1}{4}n, \dots, \frac{3}{4}n}[T(k)] + 2cn
        = 2 \mathbb{E}\left[T\left( \frac{n}{2} \right)\right] + 2cn \\
        &= 2 \left( 2 \mathbb{E}_{k \sim \frac{1}{4}\cdot\frac{n}{2}, \dots, \frac{3}{4}\cdot\frac{n}{2}}[T(k)] + 2c \cdot \frac{n}{2} \right) + 2cn
        = 2 \left( 2 \mathbb{E}\left[T\left( \frac{n}{4} \right)\right] + 2c \cdot \frac{n}{2} \right) + 2cn \\
        &= 2 \left\{ 2 \left( 2 \mathbb{E}\left[T\left( \frac{n}{8} \right)\right] + 2c \cdot \frac{n}{4} \right) + 2c \cdot \frac{n}{2} \right\} + 2cn \\
        &= 2^{3} \ \mathbb{E}\left[T\left( \frac{n}{2^{3}} \right)\right] + 2^{2} \cdot 2c \cdot \frac{n}{2^{2}} + 2^{1} \cdot 2c \cdot \frac{n}{2^{1}} + 2^{0} \cdot 2c \cdot \frac{n}{2^{0}} \\
        &\quad \vdots \\
        &\leq 2^{k} \ \mathbb{E}\left[ T\left( \frac{n}{2^{k}} \right) \right] + (k+1) \cdot 2cn
    \end{split}
\end{equation}
The recurrence ends when
\begin{equation}
    \label{rec-end}
    \frac{n}{2^{k}} = 1 \implies 2^{k} = n \implies k = \log_{2}{n}
\end{equation}
Substituting \eqref{rec-end} in \eqref{exp-Tn-mod-quicksort} gives the expected time complexity of the modified
quicksort algorithm where we only partition around good pivots.
\begin{equation}
    \label{exp-Tn-mod-quicksort-sol}
    \begin{split}
        \mathbb{E}[T(n)] &\leq 2^{\log_{2}{n}} \mathbb{E}[T(1)] + 2cn \cdot (\log_{2}{n} + 1) \\
        &= 2k cn \log_{2}{n} + (2c + 1) \cdot n \qquad \text{assuming } T(1) = 1 \\
        &\leq c^{2} \cdot n \log_{2}{n} \quad \forall \ n \geq 2 \\
        \therefore \mathbb{E}[T(n)] &= O(n \log{n})
    \end{split}
\end{equation}
Hence, the modified quicksort algorithm runs in $O(n \log{n})$ time in expectation.

\subsection*{Part (b)}
The bulk of the algorithm runtime comes from calulating the ranks of the randomly chosen pivots
to identify whether they are good, and the partitioning steps. Let
$T_{i}^{(j)} \sim \textsc{Geometric}\left(\frac{1}{2}\right)$ denote the number of trials
to find a good pivot at the $j^{th}$ branch of the $i^{th}$ level/depth of recursion. There
will be $O(\log{n})$ levels of recursion, since the size of the array goes down by at least
$\frac{3}{4}$ at each level. Then, the total work done by the algorithm is given by
\begin{equation}
    W = \sum_{i=1}^{k\log{n}} \sum_{j=1}^{2^{i-1}} c \cdot n_{i}^{(j)} T_{i}^{(j)}
\end{equation}
for some constants $c, k > 0$, where $n_{i}^{(j)}$ denotes the size of the sub-array at the $j^{th}$ branch
of the $i^{th}$ level of recursion. For a sanity check, we calculate the expectation of $W$ to verify
that it is $O(n \log{n})$.
\begin{equation}
    \mathbb{E}\left[ W \right] = c \sum_{i=1}^{k\log{n}} \sum_{j=1}^{2^{i-1}}
    \mathbb{E}\left[ n_{i}^{(j)} \right] \cdot \mathbb{E}\left[ T_{i}^{(j)} \right]
    = c \sum_{i=1}^{k\log{n}} \sum_{j=1}^{2^{i-1}} \frac{n}{2^{i}} \cdot 2
    = c \cdot n \sum_{i=1}^{k \log{n}} \sum_{j=1}^{2^{i-1}} \frac{1}{2^{i-1}}
    = c \cdot n \sum_{i=1}^{k \log{n}} 1 = ck \cdot n \log{n}
\end{equation}
where $n_{i}^{(j)}$ depends on the expected rank of the pivot (using \eqref{exp-R2}) and
we use the independence of the size of the sub-array at any step and the number of trials used
to identify a good pivot. So, the expected work done by the algorithm is $O(n \log{n})$.
\pagebreak \\
We bound the probability that (ignoring constants for simplicity)
\begin{align}
    \begin{split}
        \mathbb{P}\left[ \sum_{i=1}^{\log{n}} \sum_{j=1}^{2^{i-1}} n_{i}^{(j)} T_{i}^{(j)} > n \log{n} \right]
        &\leq \mathbb{P}\left[ \exists i : \sum_{j=1}^{2^{i-1}} T_{i}^{(j)} > 2 \log{n} \right] \\
        &\leq \sum_{i=1}^{\log{n}} \mathbb{P}\left[ \sum_{j=1}^{2^{i-1}} T_{i}^{(j)} > 2 \log{n} \right] \qquad \text{by union bound} \\
        &\leq \sum_{i=1}^{\log{n}} \mathbb{P}\left[ \exists j : T_{i}^{(j)} > 2 \log{n} \right] \\
        &\leq \sum_{i=1}^{\log{n}} \sum_{j=1}^{2^{i-1}} \mathbb{P}\left[ T_{i}^{(j)} > 2 \log{n} \right] \qquad \text{by union bound} \\\
        &= \sum_{i=1}^{\log{n}} 2^{i-1} \cdot \left( \frac{1}{2} \right)^{2 \log{n}} \\
        &= \frac{1}{n^{2}} \cdot (2^{\log{n}} - 1) \approx \frac{n}{n^{2}} = \frac{1}{n}
    \end{split}
\end{align}
where the first inequality holds as the probability that the total work done exceeds $n \log{n}$ is
at most the probability that we use more than $O(\log{n})$ trials at some level to find a good pivot.
Therefore,
\begin{equation}
    \mathbb{P}[W \leq n \log{n}] = 1 - \mathbb{P}[W > n \log{n}] \geq 1 - \frac{1}{n}
\end{equation}

\section*{Solution 2.}
\subsection*{Part (a)}
Given two vectors $\mathbf{x} = (x_{1}, x_{2}, \dots, x_{d})$ and $\mathbf{y} = (y_{1}, y_{2}, \dots, y_{d})$
chosen independently and uniformly at random from $\{-1, 1\}^{d}$. We can derive the required result using a
Chernoff bound on the inner product. For each $1 \leq i \leq d$, $x_{i}$ and $y_{i}$ are uniform random variables
taking values $\{-1, 1\}$. Let $Z_{i} = x_{i}y_{i}$ for each $i$. Then the random variable $Z_{i}$ denotes the
$i^{th}$ component of the inner product $\langle \mathbf{x}, \mathbf{y} \rangle$. Thus, the inner product can be
written as
\begin{equation}
    \langle \mathbf{x}, \mathbf{y} \rangle = \sum_{i=1}^{d} x_{i} y_{i} = \sum_{i=1}^{d} Z_{i} = Z
\end{equation}
where the random variable $Z$, the sum of $d$ independent random variables $Z_{i}$, is the inner
product of $\mathbf{x}$ and $\mathbf{y}$. We first note that
\begin{equation}
    \label{symmetry}
    \mathbb{P}[\lvert Z \rvert \geq \epsilon d] = \mathbb{P}[Z \geq \epsilon d] + \mathbb{P}[Z \leq -\epsilon d]
    = 2 \mathbb{P}[Z \geq \epsilon d]
\end{equation}
where the last equality holds by symmetry\footnote{See \textbf{Appendix} for the proof of symmetry.}.
Each $Z_{i}$ is a uniform random variable taking values $\{-1, 1\}$, as
\begin{equation}
    \begin{split}
        \mathbb{P}[Z_{i} = 1] &= \mathbb{P}[x_{i} = 1, y_{i} = 1] + \mathbb{P}[x_{i} = -1, y_{i} = -1]
        = \frac{1}{2} \cdot \frac{1}{2} + \frac{1}{2} \cdot \frac{1}{2} = \frac{1}{2} \\
        \mathbb{P}[Z_{i} = -1] &= \mathbb{P}[x_{i} = 1, y_{i} = -1] + \mathbb{P}[x_{i} = -1, y_{i} = 1]
        = \frac{1}{2} \cdot \frac{1}{2} + \frac{1}{2} \cdot \frac{1}{2} = \frac{1}{2}
    \end{split}
\end{equation}
where we crucially use the independence of $x_{i}$ and $y_{i}$. To derive a Chernoff bound, we
find the moment generating function of $Z_{i}$ ($1 \leq i \leq d$) and $Z$.
\begin{align}
    \Phi_{Z_{i}}(t) = \mathbb{E}[e^{tZ_{i}}] = \mathbb{P}[Z_{i} = 1]e^{t} + \mathbb{P}[Z_{i} = -1]e^{-t}
    &= \frac{e^{t} + e^{-t}}{2} = \cosh{t} \\
    \Phi_{Z}(t) = \prod_{i=1}^{d} \Phi_{Z_{i}}(t) = \prod_{i=1}^{d} \cosh{t} = \left( \cosh{t} \right)^{d}
    \leq \left( e^{\frac{t^{2}}{2}} \right)^{d} &= e^{\frac{t^{2}d}{2}} \quad \left(\cosh{x} \leq e^{\frac{x^{2}}{2}} \ \forall \ x \in \mathbb{R} \right)
\end{align}
using the independence of $Z_{i}$ for all $(1 \leq i \leq d)$.
\pagebreak \\
We now bound the probability of the inner product exceeding $\epsilon d$.
\begin{align}
    \begin{split}
        \mathbb{P}[Z \geq \epsilon d] = \mathbb{P}[e^{tZ} \geq e^{t\epsilon d}]
        &\leq \frac{\Phi_{Z}(t)}{e^{t \epsilon d}} \quad (t > 0) \\
        &\leq e^{-t \epsilon d} \cdot e^{\frac{t^{2}d}{2}} = e^{\frac{t^{2}d}{2} - t \epsilon d} \\
        &\leq \min_{t > 0} \ \exp{\left( \frac{t^{2}d}{2} - t \epsilon d \right)}
    \end{split}
\end{align}
where the first inequality follows by Markov's inequality. To find the minimum, we differentiate the
exponent with respect to $t$ and set it to zero.
\begin{equation}
    \frac{d}{dt} \left( \frac{t^{2}d}{2} - t \epsilon d \right) = td - \epsilon d = 0 \implies t = \epsilon
\end{equation}
Substituting $t = \epsilon$ gives
\begin{equation}
    \label{prob-Z}
    \mathbb{P}[Z \geq \epsilon d] \leq \exp{\left( \frac{\epsilon^{2}d}{2} - \epsilon^{2}d \right)}
    = \exp{\left( -\frac{\epsilon^{2}d}{2} \right)}
    \leq \exp{\left( -\frac{\epsilon^{2}d}{6} \right)}
\end{equation}
where the last inequality follows since $e^{-x}$ is monotonically decreasing.
Thus, by \eqref{symmetry} and \eqref{prob-Z}, we have
\begin{equation}
    \label{final-2a}
    \mathbb{P}[ \lvert \langle \mathbf{x}, \mathbf{y} \rangle \rvert \geq \epsilon d] \leq 2 \exp{\left(-\frac{\epsilon^{2}d}{6}\right)}
\end{equation}

\subsection*{Part (b)}
For a constant $\epsilon > 0$, a set $S$ of unit vectors is called $\epsilon$-orthonormal if\footnote{We
have slightly modified the definition from the problem, since the pairs $(\hat{\mathbf{x}}, \hat{\mathbf{x}})$ cannot be
$\epsilon$-orthogonal for any $\epsilon > 1$ otherwise, as their inner product is always 1.} for any $\hat{\mathbf{x}} \neq \hat{\mathbf{y}} \in S$,
$\lvert \langle \hat{\mathbf{x}}, \hat{\mathbf{y}} \rangle \rvert \leq \epsilon$. For a constant $c$ and $d_{0} \geq 0$,
we sample $N = e^{c \epsilon^{2} d}$ unit vectors from $\left\{ -\frac{1}{\sqrt{d}}, +\frac{1}{\sqrt{d}} \right\}^{d}$
independently and uniformly. Let
\begin{equation}
    \hat{\mathbf{x}} = \frac{1}{\sqrt{d}} \mathbf{x} = \frac{1}{\sqrt{d}} (x_{1}, x_{2}, \dots, x_{d}) \qquad \qquad
    \hat{\mathbf{y}} = \frac{1}{\sqrt{d}} \mathbf{y} = \frac{1}{\sqrt{d}} (y_{1}, y_{2}, \dots, y_{d})
\end{equation}
denote any two unit vectors in $S$, using the same notation as \textbf{Solution 2 Part (a)}.
The inner product of $\hat{\mathbf{x}}$ and $\hat{\mathbf{y}}$ is given by
\begin{equation}
    \langle \hat{\mathbf{x}}, \hat{\mathbf{y}} \rangle = \sum_{i=1}^{d} \frac{x_{i}}{\sqrt{d}} \cdot \frac{y_{i}}{\sqrt{d}} = \frac{1}{d} \sum_{i=1}^{d} Z_{i} = \frac{Z}{d}
\end{equation}
By \eqref{final-2a} (\textbf{Solution 2 Part (a)}), we see (since $d > 0$)
\begin{equation}
    \mathbb{P}[ \lvert Z \rvert \geq \epsilon d] = \mathbb{P}\left[ \left\lvert \frac{Z}{d} \right\rvert \geq \epsilon \right]
    \leq 2 \exp{\left( -\frac{\epsilon^{2}d}{6} \right)}
\end{equation}
Let us bound the probability that there is some pair of vectors in $S$ that is not $\epsilon$-orthogonal.
\begin{align}
    \begin{split}
        \mathbb{P}[\exists \hat{\mathbf{x}}, \hat{\mathbf{y}} \in S : \lvert \langle \hat{\mathbf{x}}, \hat{\mathbf{y}} \rangle \rvert \geq \epsilon]
        &\leq \binom{N}{2} \ \mathbb{P}[ \lvert \langle \hat{\mathbf{x}}, \hat{\mathbf{y}} \rangle \rvert \geq \epsilon] \quad \text{by union bound} \\
        &= \frac{N(N-1)}{2} \cdot 2 \exp{\left( -\frac{\epsilon^{2}d}{6} \right)} \\
        &\leq N^{2} \exp{\left( -\frac{\epsilon^{2}d}{6} \right)}
        = \exp{\left( 2c \epsilon^{2} d -\frac{\epsilon^{2}d}{6} \right)} \\
    \end{split}
\end{align}
We want that such a pair with a large inner product exists with probability at most half. So,
\begin{align}
    \begin{split}
        \exp{\left( 2c \epsilon^{2} d -\frac{\epsilon^{2}d}{6} \right)} &\leq \frac{1}{2} \\
        2c \epsilon^{2} d -\frac{\epsilon^{2}d}{6} &\leq -\ln{2} \\
        c &\leq \frac{1}{12} - \frac{1}{2\epsilon^{2}d} \ln{2}
    \end{split}
\end{align}
Since we sample $N = e^{c \epsilon^{2} d}$ vectors, $c \geq 0$. Thus, we want
\begin{equation}
    \frac{1}{12} \gg \frac{1}{2\epsilon^{2}d} \ln{2} \implies d \gg \frac{6}{\epsilon^{2}} \ln{2}
\end{equation}
Since we just want to prove the existence of such constants $c$ and $d_{0}$, we can choose
\begin{equation}
    d \geq d_{0} = \frac{60}{\epsilon^{2}} \ln{2} \implies c \leq \frac{1}{12} - \frac{1}{120} = \frac{3}{40}
\end{equation}
Therefore, for $d_{0} = \frac{60}{\epsilon^{2}} \ln{2}$, we can sample $N = e^{\frac{3}{40} \epsilon^{2} d}$
unit vectors in $d \geq d_{0}$ dimensions such that with probability at least half, the set of vectors is
$\epsilon$-orthonormal.

\section*{Appendix}
\subsection*{Solution 2 (a)}
The derivation for the bound on the lower tail of the inner product is given here.
\begin{align}
    \begin{split}
        \mathbb{P}[Z \leq -\epsilon d] = \mathbb{P}[e^{tZ} \geq e^{-t\epsilon d}]
        &\leq \frac{\Phi_{Z}(t)}{e^{-t \epsilon d}} \quad (t < 0) \\
        &\leq e^{t \epsilon d} \cdot e^{\frac{t^{2}d}{2}} = e^{\frac{t^{2}d}{2} + t \epsilon d} \\
        &\leq \min_{t < 0} \ \exp{\left( \frac{t^{2}d}{2} + t \epsilon d \right)}
    \end{split}
\end{align}
We differentiate with respect to $t$ and set it to zero to find the minimum.
\begin{equation}
    \frac{d}{dt} \left( \frac{t^{2}d}{2} + t \epsilon d \right) = td + \epsilon d = 0 \implies t = -\epsilon
\end{equation}
Substituting $t = -\epsilon$ gives
\begin{equation}
    \mathbb{P}[Z \leq -\epsilon d] \leq \exp{\left( \frac{\epsilon^{2}d}{2} - \epsilon^{2}d \right)}
    = \exp{\left( -\frac{\epsilon^{2}d}{2} \right)}
    \leq \exp{\left( -\frac{\epsilon^{2}d}{6} \right)}
\end{equation}
which is the same as the derived bound of the upper tail.

\end{document}